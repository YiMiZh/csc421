{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "66770b88-2e5c-4d94-b291-c2ee9baac8cc"
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "  Using cached https://files.pythonhosted.org/packages/0d/f3/421598450cb9503f4565d936860763b5af413a61009d87a5ab1e34139672/Pillow-5.4.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.4.1\n",
            "    Uninstalling Pillow-5.4.1:\n",
            "      Successfully uninstalled Pillow-5.4.1\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper code"
      ]
    },
    {
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ]
    },
    {
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ]
    },
    {
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXNsLNkOn38w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "metadata": {
        "id": "_BAfi_8yWB3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GRU cell"
      ]
    },
    {
      "metadata": {
        "id": "9ztmyA5Ro67o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = F.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = F.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = F.tanh(self.Wih(x) + r * self.Whh(h_prev))\n",
        "        h_new = (1 - z) * g + z * h_prev\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JBVFLEZWNC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GRU encoder / decoder"
      ]
    },
    {
      "metadata": {
        "id": "xaDt7XDmWRzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWe0RO5FWajD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ]
    },
    {
      "metadata": {
        "id": "9GUK5A7CWhV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size()[0]\n",
        "        seq_len = keys.size()[1]\n",
        "        hidden_size = keys.size()[2]\n",
        "        expanded_queries = torch.unsqueeze(queries, 1).expand_as(keys)\n",
        "        concat_inputs = torch.cat((expanded_queries, keys), 2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs.view(batch_size * seq_len, 2 * hidden_size)).view(batch_size, seq_len, 1)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.view(batch_size, 1, seq_len), values)\n",
        "        return context, attention_weights\n",
        "        \n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size()[0]\n",
        "        seq_len = keys.size()[1]\n",
        "        hidden_size = keys.size()[2]\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        if len(q.size()) == 2:\n",
        "            q = q.view(batch_size, 1, hidden_size)\n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q, 1, 2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(torch.transpose(attention_weights, 1, 2), v)\n",
        "        return context, attention_weights\n",
        "        \n",
        "\n",
        "      \n",
        "      \n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size()[0]\n",
        "        seq_len = keys.size()[1]\n",
        "        hidden_size = keys.size()[2]\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        if len(q.size()) == 2:\n",
        "            q = q.view(batch_size, 1, hidden_size)\n",
        "        k_size = q.size()[1]\n",
        "        unnormalized_attention = torch.bmm(q, torch.transpose(k,1,2)) * self.scaling_factor\n",
        "        mask = torch.unsqueeze(torch.tril(torch.ones((seq_len, k_size), dtype=torch.uint8), diagonal=k_size-seq_len).cuda(), 0).expand(batch_size, -1, -1)\n",
        "        unnormalized_attention = unnormalized_attention.masked_fill(mask, self.neg_inf)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(torch.transpose(attention_weights,1,2), v)\n",
        "        return context, attention_weights\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pemjZo2XWtRt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "PfjF0Z-PWwPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN\n",
        "            # ------------\n",
        "            embed_current = embed[:,i,:].squeeze(1)\n",
        "            context, attention_weights = self.attention.forward(h_prev, annotations, annotations)\n",
        "            embed_and_context = torch.cat((context.squeeze(1), embed_current), 1) \n",
        "            h_prev = self.rnn.forward(embed_and_context, h_prev)\n",
        "\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N8JpcwTRW5cw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformer decoder"
      ]
    },
    {
      "metadata": {
        "id": "V5vJPku1W7sz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "#         self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "#                                     hidden_size=hidden_size, \n",
        "#                                  ) for i in range(self.num_layers)])\n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN\n",
        "          # ------------\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
        "          residual_contexts = contexts + new_contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = residual_contexts + new_contexts + contexts\n",
        "\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuNFd6LNo0-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ]
    },
    {
      "metadata": {
        "id": "kiUwiOITHTW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ]
    },
    {
      "metadata": {
        "id": "xwcFjsEpHRbI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40817bf8-d214-499c-9d35-9f4423cf3510"
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmQmyJDSRFKR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1516
        },
        "outputId": "c94d359c-7e2c-4c13-a698-9e988f692281"
      },
      "cell_type": "code",
      "source": [
        "# TEST_SENTENCE = 'the air conditioning is working'\n",
        "TEST_SENTENCE = 'my name is yiming zhang'\n",
        "# TEST_SENTENCE = 'today i went to shopping'\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.435 | Val loss: 2.022 | Gen: ay ay eray ensay ay \n",
            "Epoch:   1 | Train loss: 1.937 | Val loss: 1.849 | Gen: ay ay ongsay ongeday ay\n",
            "Epoch:   2 | Train loss: 1.780 | Val loss: 1.748 | Gen: onstay arestay ingsay ontingway aredway\n",
            "Epoch:   3 | Train loss: 1.677 | Val loss: 1.677 | Gen: way artay ingsay ingstay artay\n",
            "Epoch:   4 | Train loss: 1.601 | Val loss: 1.624 | Gen: y aredway ingsay ingstay artay\n",
            "Epoch:   5 | Train loss: 1.539 | Val loss: 1.575 | Gen: ytay aredway ingsay ingstay anghay\n",
            "Epoch:   6 | Train loss: 1.486 | Val loss: 1.537 | Gen: yay antedway ingsay ingday antway\n",
            "Epoch:   7 | Train loss: 1.438 | Val loss: 1.504 | Gen: yay antedway istingsay ingsay antway\n",
            "Epoch:   8 | Train loss: 1.397 | Val loss: 1.477 | Gen: yay antedway isssay ingway antway\n",
            "Epoch:   9 | Train loss: 1.358 | Val loss: 1.462 | Gen: yay antedway isssay ingtay anghay\n",
            "Epoch:  10 | Train loss: 1.326 | Val loss: 1.433 | Gen: ytay antedway isssay ingtay anghay\n",
            "Epoch:  11 | Train loss: 1.292 | Val loss: 1.422 | Gen: ytay antedway isssay ingtay angtay\n",
            "Epoch:  12 | Train loss: 1.262 | Val loss: 1.396 | Gen: ytay anestay isssay ingtay angthay\n",
            "Epoch:  13 | Train loss: 1.239 | Val loss: 1.412 | Gen: ytay anessay istray intyssay angsay\n",
            "Epoch:  14 | Train loss: 1.226 | Val loss: 1.374 | Gen: y-ayday anestay istray ingtay angshay\n",
            "Epoch:  15 | Train loss: 1.209 | Val loss: 1.395 | Gen: ytay anedway issay intingway anghay\n",
            "Epoch:  16 | Train loss: 1.194 | Val loss: 1.363 | Gen: ytay anestay issay ithingway anghay\n",
            "Epoch:  17 | Train loss: 1.158 | Val loss: 1.362 | Gen: y-ayday anessay issay ithingway angsay\n",
            "Epoch:  18 | Train loss: 1.139 | Val loss: 1.330 | Gen: y-ayday anestay istray ithingway angsay\n",
            "Epoch:  19 | Train loss: 1.128 | Val loss: 1.324 | Gen: y-ayday anessay isstay itay-ingway anghay\n",
            "Epoch:  20 | Train loss: 1.109 | Val loss: 1.314 | Gen: y-ayday amentsay isstay itay-ingway angsay\n",
            "Epoch:  21 | Train loss: 1.096 | Val loss: 1.315 | Gen: ysay amentsay isstay iationdway anghay\n",
            "Epoch:  22 | Train loss: 1.101 | Val loss: 1.312 | Gen: y-ayday amentsay isstay iationway anghay\n",
            "Epoch:  23 | Train loss: 1.084 | Val loss: 1.278 | Gen: ytay amentsay isshay iationday anghay\n",
            "Epoch:  24 | Train loss: 1.066 | Val loss: 1.269 | Gen: yway amentway isshay iationday anghay\n",
            "Epoch:  25 | Train loss: 1.045 | Val loss: 1.249 | Gen: y-ayday amentsay isshay iationday anghay\n",
            "Epoch:  26 | Train loss: 1.033 | Val loss: 1.248 | Gen: yway amentsay isshay iationway anghay\n",
            "Epoch:  27 | Train loss: 1.019 | Val loss: 1.248 | Gen: ypay amentway isshay iationday anghay\n",
            "Epoch:  28 | Train loss: 1.019 | Val loss: 1.231 | Gen: yway amestay isshay iaygay anghay\n",
            "Epoch:  29 | Train loss: 1.022 | Val loss: 1.248 | Gen: ypay amentway isshay iationway anghay\n",
            "Epoch:  30 | Train loss: 1.016 | Val loss: 1.213 | Gen: yway amestay isshay iationway anghay\n",
            "Epoch:  31 | Train loss: 0.989 | Val loss: 1.205 | Gen: yway amentway isshay iinghay anghay\n",
            "Epoch:  32 | Train loss: 0.986 | Val loss: 1.207 | Gen: yway amentway isshay iinghay anthsay\n",
            "Epoch:  33 | Train loss: 0.995 | Val loss: 1.194 | Gen: yway amentway isshay yintingway anthsay\n",
            "Epoch:  34 | Train loss: 0.957 | Val loss: 1.185 | Gen: yway amesthay isshay yintingway antshay\n",
            "Epoch:  35 | Train loss: 0.948 | Val loss: 1.183 | Gen: yway amentway isshay yintingway antshay\n",
            "Epoch:  36 | Train loss: 0.939 | Val loss: 1.172 | Gen: yway amentway isshay yintingway antshay\n",
            "Epoch:  37 | Train loss: 0.946 | Val loss: 1.170 | Gen: yway amesthay isshay yintingway anthway\n",
            "Epoch:  38 | Train loss: 0.937 | Val loss: 1.169 | Gen: yway amengway isshay yimingway antshay\n",
            "Epoch:  39 | Train loss: 0.942 | Val loss: 1.168 | Gen: yway amestray isshay yimingway anghsay\n",
            "Epoch:  40 | Train loss: 0.945 | Val loss: 1.155 | Gen: ybay amentway isshay yintingway antshay\n",
            "Epoch:  41 | Train loss: 0.917 | Val loss: 1.159 | Gen: yway amemedway isshay yintionway anghay\n",
            "Epoch:  42 | Train loss: 0.906 | Val loss: 1.151 | Gen: yway amesthay isshay yintingway anthway\n",
            "Epoch:  43 | Train loss: 0.902 | Val loss: 1.151 | Gen: yway amengway isshay yimingway antshay\n",
            "Epoch:  44 | Train loss: 0.885 | Val loss: 1.139 | Gen: yway amestay isshay yintingway anthway\n",
            "Epoch:  45 | Train loss: 0.870 | Val loss: 1.137 | Gen: yway amesthay isshay yintingway antshay\n",
            "Epoch:  46 | Train loss: 0.871 | Val loss: 1.126 | Gen: yway amesthay isshay yintingway antshay\n",
            "Epoch:  47 | Train loss: 0.859 | Val loss: 1.135 | Gen: yway amengway isshay yintingway anthway\n",
            "Epoch:  48 | Train loss: 0.878 | Val loss: 1.131 | Gen: yway amentway isshay yinitway anthway\n",
            "Epoch:  49 | Train loss: 0.868 | Val loss: 1.127 | Gen: yway amengcay isshay yinithway antshay\n",
            "Epoch:  50 | Train loss: 0.846 | Val loss: 1.109 | Gen: yway amestay issay yinithway anthway\n",
            "Epoch:  51 | Train loss: 0.844 | Val loss: 1.110 | Gen: yway amengedway issay yinghay-inway-awlay anghay\n",
            "Epoch:  52 | Train loss: 0.842 | Val loss: 1.109 | Gen: yway amengway issay yintingway anthway\n",
            "Epoch:  53 | Train loss: 0.835 | Val loss: 1.092 | Gen: yway amengedway issay yingray-inway-awlay anghay\n",
            "Epoch:  54 | Train loss: 0.830 | Val loss: 1.114 | Gen: yway amestay issay yinghay-inway-awlay anghay\n",
            "Epoch:  55 | Train loss: 0.839 | Val loss: 1.101 | Gen: yway amengengay issay yinghay-inway-awlay anghay\n",
            "Epoch:  56 | Train loss: 0.826 | Val loss: 1.084 | Gen: yway amengedway issay yinithway anghay\n",
            "Epoch:  57 | Train loss: 0.809 | Val loss: 1.087 | Gen: yway amengengway issay yinghay-inway-awlay anghay\n",
            "Exiting early from training.\n",
            "source:\t\tmy name is yiming zhang \n",
            "translated:\tyway amengway issay yinghay-inway-awlay anghay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2kPGj5DFv7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f5fc9a61-45f6-401c-b7e2-bd54740b25b6"
      },
      "cell_type": "code",
      "source": [
        "# TEST_SENTENCE = 'the air conditioning is working'\n",
        "TEST_SENTENCE = 'my name is yiming zhang'\n",
        "# TEST_SENTENCE = 'today i went to shopping'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tmy name is yiming zhang \n",
            "translated:\tyway amengway issay yinghay-inway-awlay anghay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7cP7nl5NRJbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "nKlyfbuPDXDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2213
        },
        "outputId": "27e513d4-f534-412d-ebd2-43ade25faee1"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "# TEST_SENTENCE = 'my name is yiming zhang'\n",
        "# TEST_SENTENCE = 'today i went to shopping'\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              # 'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "              'attention_type': 'scaled_dot',\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: scaled_dot                             \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.396 | Val loss: 1.996 | Gen: ereray ay oereray eray ongtay\n",
            "Epoch:   1 | Train loss: 1.868 | Val loss: 1.702 | Gen: entay anday ontinghay insay ontinghay\n",
            "Epoch:   2 | Train loss: 1.610 | Val loss: 1.526 | Gen: entay andway ontinghay inssay onthay\n",
            "Epoch:   3 | Train loss: 1.435 | Val loss: 1.399 | Gen: eedtay atedway ontiontay ishay ontingway\n",
            "Epoch:   4 | Train loss: 1.299 | Val loss: 1.247 | Gen: eay atray ontiontay ishay ortingway\n",
            "Epoch:   5 | Train loss: 1.137 | Val loss: 1.088 | Gen: elay ateray onditationgway ishay orsinghay\n",
            "Epoch:   6 | Train loss: 0.938 | Val loss: 0.957 | Gen: eray airsay onditiongway issay orkingsay\n",
            "Epoch:   7 | Train loss: 0.804 | Val loss: 0.804 | Gen: elay airsay onditiongway issay orkingsay\n",
            "Epoch:   8 | Train loss: 0.722 | Val loss: 0.719 | Gen: eway airsray ondiontatingway issay orkingsay\n",
            "Epoch:   9 | Train loss: 0.717 | Val loss: 1.101 | Gen: ehay airstay ondingay ishay orkingsay\n",
            "Epoch:  10 | Train loss: 0.867 | Val loss: 0.770 | Gen: elay airsray onditiongmay ishay orkingsay\n",
            "Epoch:  11 | Train loss: 0.641 | Val loss: 0.672 | Gen: eway airway onditiongnay ishay orkingsay\n",
            "Epoch:  12 | Train loss: 0.528 | Val loss: 0.624 | Gen: egay airway onditiongnay isishay orkingsay\n",
            "Epoch:  13 | Train loss: 0.519 | Val loss: 0.676 | Gen: egay airray onditatingay isascay orkingway\n",
            "Epoch:  14 | Train loss: 0.498 | Val loss: 0.589 | Gen: etay airtay onditiongnay ishay orkingway\n",
            "Epoch:  15 | Train loss: 0.429 | Val loss: 0.494 | Gen: etay airfay onditiongway ishay orkingsay\n",
            "Epoch:  16 | Train loss: 0.399 | Val loss: 0.518 | Gen: etay airfay onditiongnay ishay orkingway\n",
            "Epoch:  17 | Train loss: 0.557 | Val loss: 0.514 | Gen: etay airway onditiongway ishay orkingway\n",
            "Epoch:  18 | Train loss: 0.421 | Val loss: 0.498 | Gen: etay airray onditioningay ishbay orkingsay\n",
            "Epoch:  19 | Train loss: 0.443 | Val loss: 0.482 | Gen: etay airway onditiongway ishay orkingway\n",
            "Epoch:  20 | Train loss: 0.374 | Val loss: 0.454 | Gen: etay airway onditiongnay isday orkingway\n",
            "Epoch:  21 | Train loss: 0.417 | Val loss: 0.450 | Gen: etay airway onditiongway ishay orkingway\n",
            "Epoch:  22 | Train loss: 0.347 | Val loss: 0.435 | Gen: etay airway onditiongway isfay orkingway\n",
            "Epoch:  23 | Train loss: 0.479 | Val loss: 0.684 | Gen: etay airway onditiongmay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.489 | Val loss: 0.502 | Gen: etay airway onditiongpay ishay orkingsay\n",
            "Epoch:  25 | Train loss: 0.412 | Val loss: 0.501 | Gen: etay airway onditiongnay isday orkingway\n",
            "Epoch:  26 | Train loss: 0.361 | Val loss: 0.443 | Gen: etay airway onditiongway isachay orkingsay\n",
            "Epoch:  27 | Train loss: 0.344 | Val loss: 0.450 | Gen: etay airway onditiongday isay orkingsay\n",
            "Epoch:  28 | Train loss: 0.326 | Val loss: 0.430 | Gen: etay airway onditiongway isway orkingsay\n",
            "Epoch:  29 | Train loss: 0.320 | Val loss: 0.429 | Gen: etay airway onditiongday ishay orkingsay\n",
            "Epoch:  30 | Train loss: 0.282 | Val loss: 0.411 | Gen: etay airday onditioningway isway orkingsay\n",
            "Epoch:  31 | Train loss: 0.262 | Val loss: 0.400 | Gen: etay airway onditiongday isay orkingsay\n",
            "Epoch:  32 | Train loss: 0.253 | Val loss: 0.383 | Gen: etay airday onditiongray isway orkingway\n",
            "Epoch:  33 | Train loss: 0.243 | Val loss: 0.370 | Gen: etay airway onditiongray isway orkingway\n",
            "Epoch:  34 | Train loss: 0.235 | Val loss: 0.376 | Gen: etay airday onditiongray isway orkingsay\n",
            "Epoch:  35 | Train loss: 0.235 | Val loss: 0.373 | Gen: etay airway onditiongnay isway orkingsay\n",
            "Epoch:  36 | Train loss: 0.229 | Val loss: 0.373 | Gen: etay airway onditioningway isway orkingsay\n",
            "Epoch:  37 | Train loss: 0.393 | Val loss: 0.632 | Gen: etay airray onditiongnay ishay orkingway\n",
            "Epoch:  38 | Train loss: 0.409 | Val loss: 0.423 | Gen: enay airway onditionlnay isway orkingsay\n",
            "Epoch:  39 | Train loss: 0.263 | Val loss: 0.383 | Gen: etay airway onditioningpray ishay orkingsay\n",
            "Epoch:  40 | Train loss: 0.234 | Val loss: 0.367 | Gen: etay airway onditiongcay isway orkingsay\n",
            "Epoch:  41 | Train loss: 0.215 | Val loss: 0.339 | Gen: etay airway onditiongnay isway orkingsay\n",
            "Epoch:  42 | Train loss: 0.204 | Val loss: 0.321 | Gen: etay airway onditioningray isway orkingsay\n",
            "Epoch:  43 | Train loss: 0.200 | Val loss: 0.319 | Gen: etay airway onditioningray isway orkingsay\n",
            "Epoch:  44 | Train loss: 0.192 | Val loss: 0.325 | Gen: etay airway onditioningray isway orkingway\n",
            "Epoch:  45 | Train loss: 0.188 | Val loss: 0.320 | Gen: etay airway onditioningray isway orkingway\n",
            "Epoch:  46 | Train loss: 0.189 | Val loss: 0.336 | Gen: etay airway onditioningray isway orkingway\n",
            "Epoch:  47 | Train loss: 0.284 | Val loss: 0.398 | Gen: etay airway onditiongray isway orkingsay\n",
            "Epoch:  48 | Train loss: 0.236 | Val loss: 0.351 | Gen: etay airway onditioningcay isway orkingsay\n",
            "Epoch:  49 | Train loss: 0.331 | Val loss: 0.479 | Gen: etay airhay onditiongcay isay orkingway\n",
            "Epoch:  50 | Train loss: 0.328 | Val loss: 0.517 | Gen: etay airway onditionlnay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.253 | Val loss: 0.321 | Gen: etay airway onditioningcay isway orkingshay\n",
            "Epoch:  52 | Train loss: 0.266 | Val loss: 0.427 | Gen: etay airway onditiongnay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.278 | Val loss: 0.543 | Gen: etay airray onditiongnay isway orkingsay\n",
            "Epoch:  54 | Train loss: 0.288 | Val loss: 0.317 | Gen: etay airway onditioningray isway orkingsay\n",
            "Epoch:  55 | Train loss: 0.193 | Val loss: 0.297 | Gen: etay airway onditioningray isway orkingsay\n",
            "Epoch:  56 | Train loss: 0.172 | Val loss: 0.291 | Gen: ethay airway onditioningray isway orkingsay\n",
            "Epoch:  57 | Train loss: 0.163 | Val loss: 0.287 | Gen: ethay airway onditioningray isway orkingway\n",
            "Epoch:  58 | Train loss: 0.155 | Val loss: 0.286 | Gen: ethay airway onditioningray isway orkingway\n",
            "Epoch:  59 | Train loss: 0.148 | Val loss: 0.283 | Gen: ethay airway onditioningray isway orkingway\n",
            "Epoch:  60 | Train loss: 0.142 | Val loss: 0.277 | Gen: ethay airway onditioningray isway orkingway\n",
            "Epoch:  61 | Train loss: 0.151 | Val loss: 0.317 | Gen: etay airway onditioningray isway orkingway\n",
            "Epoch:  62 | Train loss: 0.342 | Val loss: 0.426 | Gen: etay airway onditiongnay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.228 | Val loss: 0.375 | Gen: ethay airway onditioningray isway orkingway\n",
            "Epoch:  64 | Train loss: 0.193 | Val loss: 0.298 | Gen: etay airway onditiongnay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.144 | Val loss: 0.259 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.130 | Val loss: 0.253 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.124 | Val loss: 0.254 | Gen: ethay airway onditiongray isway orkingway\n",
            "Epoch:  68 | Train loss: 0.118 | Val loss: 0.252 | Gen: ethay airway onditiongray isway orkingway\n",
            "Epoch:  69 | Train loss: 0.114 | Val loss: 0.256 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.118 | Val loss: 0.273 | Gen: ethay airway onditioningray isway orkingway\n",
            "Epoch:  71 | Train loss: 0.130 | Val loss: 0.268 | Gen: etay airway onditiongray isway orkingway\n",
            "Epoch:  72 | Train loss: 0.122 | Val loss: 0.266 | Gen: ethay airway onditioningray isway orkingway\n",
            "Epoch:  73 | Train loss: 0.114 | Val loss: 0.273 | Gen: ethay airway onditiongray isway orkingway\n",
            "Epoch:  74 | Train loss: 0.108 | Val loss: 0.253 | Gen: ethay airway onditionlnay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.168 | Val loss: 0.822 | Gen: ethay airway onditioningcayittint isway orkingway\n",
            "Epoch:  76 | Train loss: 0.419 | Val loss: 0.333 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.513 | Val loss: 0.468 | Gen: etay airway ondititnnay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.247 | Val loss: 0.296 | Gen: etay airway onditiongcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.163 | Val loss: 0.264 | Gen: etay airway onditiongcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.136 | Val loss: 0.256 | Gen: etay airway onditiongcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.122 | Val loss: 0.253 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.112 | Val loss: 0.236 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.105 | Val loss: 0.233 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.099 | Val loss: 0.233 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.094 | Val loss: 0.226 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.089 | Val loss: 0.222 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.085 | Val loss: 0.217 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.082 | Val loss: 0.220 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.079 | Val loss: 0.210 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.076 | Val loss: 0.215 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.073 | Val loss: 0.207 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.071 | Val loss: 0.213 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.069 | Val loss: 0.213 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.075 | Val loss: 0.197 | Gen: ethay airway onditiongray isway orkingway\n",
            "Epoch:  95 | Train loss: 0.264 | Val loss: 0.567 | Gen: ethay airway onditiongray isway orkingway\n",
            "Epoch:  96 | Train loss: 0.272 | Val loss: 0.302 | Gen: etay airway onditiongcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.131 | Val loss: 0.223 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.089 | Val loss: 0.211 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.078 | Val loss: 0.207 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vE-hKCxhF3iR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3bce80f0-6e89-4704-8d3a-b4282755f4d8"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8FaZZUWRpY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ]
    },
    {
      "metadata": {
        "id": "Ik5rx9qw9KCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2213
        },
        "outputId": "e96efa34-8e1d-4f9d-9936-da5689805272"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "# TEST_SENTENCE = 'my name is yiming zhang'\n",
        "# TEST_SENTENCE = 'today i went to shopping'\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                 num_transformer_layers: 3                                      \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: transformer                            \n",
            "                               lr_decay: 0.99                                   \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.782 | Val loss: 1.838 | Gen: eeeeeeeeeeeeeeeeeeee  indindindindindindin  -\n",
            "Epoch:   1 | Train loss: 1.665 | Val loss: 1.481 | Gen: -------------------- -------------------- -------------------- isssssssssssssssssss --------------------\n",
            "Epoch:   2 | Train loss: 1.323 | Val loss: 1.142 | Gen: elelelelelelelelelel aiaaiaaiaairaiiaaiia ifififififififififif iscay \n",
            "Epoch:   3 | Train loss: 1.065 | Val loss: 0.974 | Gen: -------------------- ay -------------------- isssssssssssssssssss --------------------\n",
            "Epoch:   4 | Train loss: 0.898 | Val loss: 0.834 | Gen: ehaay ay itay isisisisisisisisisis ingwagwy\n",
            "Epoch:   5 | Train loss: 0.728 | Val loss: 0.752 | Gen: ewaay airairairairairairai itititititititititit isayayayayay inghaygy\n",
            "Epoch:   6 | Train loss: 0.655 | Val loss: 0.711 | Gen: etqit ay g isday orking\n",
            "Epoch:   7 | Train loss: 0.601 | Val loss: 0.693 | Gen: ethay airairairairairairai ondtyEOSiyyyyyyyy isayayayayayayay orkinghay\n",
            "Epoch:   8 | Train loss: 0.573 | Val loss: 0.640 | Gen: ecaay  ondinninninninninnin isday orkingwayEOSway\n",
            "Epoch:   9 | Train loss: 0.527 | Val loss: 0.610 | Gen: eay  ong ishay orkingway\n",
            "Epoch:  10 | Train loss: 0.505 | Val loss: 0.601 | Gen:   ondcay ishay orkingway\n",
            "Epoch:  11 | Train loss: 0.493 | Val loss: 0.561 | Gen: echay airairairairairairar ongcay ishay orkingway\n",
            "Epoch:  12 | Train loss: 0.517 | Val loss: 0.599 | Gen: ethay  oncayEOSiEOSy ishay orkingway\n",
            "Epoch:  13 | Train loss: 0.483 | Val loss: 0.590 | Gen: ecaay  ony ishay orgway\n",
            "Epoch:  14 | Train loss: 0.498 | Val loss: 0.569 | Gen: ecaay  onityty ishay orkinghay\n",
            "Epoch:  15 | Train loss: 0.492 | Val loss: 0.534 | Gen: ethay  ong ishay orkingway\n",
            "Epoch:  16 | Train loss: 0.433 | Val loss: 0.507 | Gen: ethay  oncnyny ishay orkwagy\n",
            "Epoch:  17 | Train loss: 0.424 | Val loss: 0.517 | Gen: ecaay ayEOSaay ondiondiondiondiondi isway orkingway\n",
            "Epoch:  18 | Train loss: 0.425 | Val loss: 0.494 | Gen: ethay - ondnyny ishay orkingway\n",
            "Epoch:  19 | Train loss: 0.435 | Val loss: 0.523 | Gen: echay  ondindindindindindin isayayay orkingway\n",
            "Epoch:  20 | Train loss: 0.439 | Val loss: 0.521 | Gen: ethay  ondioniioiiiiiiiiiii isay orkingway\n",
            "Epoch:  21 | Train loss: 0.411 | Val loss: 0.497 | Gen: ethay airairairairairairaa ondiondiondiondiondi isday orky\n",
            "Epoch:  22 | Train loss: 0.409 | Val loss: 0.484 | Gen: ethay  oncayEOScay isway orky\n",
            "Epoch:  23 | Train loss: 0.390 | Val loss: 0.467 | Gen: ethay  ondioniioiiiiiiiiiii iswayEOSy orkingway\n",
            "Epoch:  24 | Train loss: 0.392 | Val loss: 0.475 | Gen: ethay  ongcnnccnccccccccccc isway orkiny\n",
            "Epoch:  25 | Train loss: 0.397 | Val loss: 0.483 | Gen: ety airwairwairwairwairw oncay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.385 | Val loss: 0.467 | Gen: ethay  ondindindindindindin isway orkiagy\n",
            "Epoch:  27 | Train loss: 0.370 | Val loss: 0.450 | Gen: ethay  ondiondiondiondiondi isway orkingway\n",
            "Epoch:  28 | Train loss: 0.385 | Val loss: 0.470 | Gen: ethay  ondiondiodiodiodiodi isway orkiagy\n",
            "Epoch:  29 | Train loss: 0.378 | Val loss: 0.463 | Gen: ety airwairwairwairwaira ondinninninninninnin isway orkiny\n",
            "Epoch:  30 | Train loss: 0.376 | Val loss: 0.469 | Gen: ethay airwairwairwairwaira ondiondiondiondionio isway orkingy\n",
            "Epoch:  31 | Train loss: 0.376 | Val loss: 0.445 | Gen: ethay - oncayEOSy isway orkinay\n",
            "Epoch:  32 | Train loss: 0.364 | Val loss: 0.445 | Gen:  airwairwairwairwaira ondiondiondiondiondo isway orkingy\n",
            "Epoch:  33 | Train loss: 0.368 | Val loss: 0.457 | Gen: ety airwairwairwairwaira ondiondiondiondionio isway orkingwy\n",
            "Epoch:  34 | Train loss: 0.399 | Val loss: 0.471 | Gen: ethay airwairwairwairwaira ondiondiodiodiodiodi isway orkiny\n",
            "Epoch:  35 | Train loss: 0.370 | Val loss: 0.454 | Gen: ethay  ondionoioioioiooiooi isway orkingy\n",
            "Epoch:  36 | Train loss: 0.376 | Val loss: 0.453 | Gen:   onionionioioioioioio isway orkingwayy\n",
            "Epoch:  37 | Train loss: 0.374 | Val loss: 0.454 | Gen: ethay airwairwairwairwaira ondionionionionionio isway ory\n",
            "Epoch:  38 | Train loss: 0.372 | Val loss: 0.433 | Gen:   ondiondiondiondionio isway orkingway\n",
            "Epoch:  39 | Train loss: 0.350 | Val loss: 0.441 | Gen: ethay  ondiondiodiodiodiodi isway orkingway\n",
            "Epoch:  40 | Train loss: 0.374 | Val loss: 0.455 | Gen:  airairairairairairai ondiondiondindindind isway orkingway\n",
            "Epoch:  41 | Train loss: 0.359 | Val loss: 0.436 | Gen: ethy  ondiodiioiioiioiioii isway orkingway\n",
            "Epoch:  42 | Train loss: 0.361 | Val loss: 0.447 | Gen: ethay airwairwairwairwaira ondiondiondiondiondi isway orkingway\n",
            "Epoch:  43 | Train loss: 0.355 | Val loss: 0.438 | Gen: ethay  ondititititititittit isway orkingway\n",
            "Epoch:  44 | Train loss: 0.350 | Val loss: 0.442 | Gen:   ondiondiondiondionio isway orkingway\n",
            "Epoch:  45 | Train loss: 0.339 | Val loss: 0.417 | Gen: ethay  oncayEOSay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.331 | Val loss: 0.419 | Gen:   ondiondiondiondionio isway orkingway\n",
            "Epoch:  47 | Train loss: 0.335 | Val loss: 0.427 | Gen: ethy airwairwairwairwaira onininininininininni isway orkingway\n",
            "Epoch:  48 | Train loss: 0.337 | Val loss: 0.430 | Gen: ethy  onininininininininin isway ory\n",
            "Epoch:  49 | Train loss: 0.338 | Val loss: 0.429 | Gen:  ayraarraarraarraarra oncay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.348 | Val loss: 0.425 | Gen: ethay  onininininininininin isway orkingway\n",
            "Epoch:  51 | Train loss: 0.342 | Val loss: 0.418 | Gen:   ondoiioodididididiid isway orkingway\n",
            "Epoch:  52 | Train loss: 0.337 | Val loss: 0.427 | Gen: etay airwairwairwairwaiwa ondcayEOSy isway ory\n",
            "Epoch:  53 | Train loss: 0.327 | Val loss: 0.412 | Gen:   ondititititititiitit isway orkingway\n",
            "Epoch:  54 | Train loss: 0.336 | Val loss: 0.416 | Gen:  airairairairairairar onionionionionononon isway orkingway\n",
            "Epoch:  55 | Train loss: 0.330 | Val loss: 0.416 | Gen: ety  ondindindindindindin isway orkingway\n",
            "Epoch:  56 | Train loss: 0.336 | Val loss: 0.414 | Gen:   onininininininininin isway orkingway\n",
            "Epoch:  57 | Train loss: 0.332 | Val loss: 0.417 | Gen: ethy  onnnnnnnnnnnnnnnnnnn isway ory\n",
            "Epoch:  58 | Train loss: 0.329 | Val loss: 0.412 | Gen: ethy  onininininininininni isway orkingway\n",
            "Epoch:  59 | Train loss: 0.329 | Val loss: 0.413 | Gen: ethy airwairwairwairwaira ondindindindindindin isway orkingway\n",
            "Epoch:  60 | Train loss: 0.330 | Val loss: 0.422 | Gen: ethy  ondcayEOSyy isway orky\n",
            "Epoch:  61 | Train loss: 0.336 | Val loss: 0.401 | Gen:   onininininininininni isway orky\n",
            "Epoch:  62 | Train loss: 0.323 | Val loss: 0.404 | Gen:   onnnnnnnnnnnnnnnnnnn  orky\n",
            "Epoch:  63 | Train loss: 0.324 | Val loss: 0.401 | Gen: ethay  ondindindindindindin isway orkingway\n",
            "Epoch:  64 | Train loss: 0.326 | Val loss: 0.409 | Gen: ethay airwairwairwairwaira onnnnnnnnnnnnnnnnnnn isway orkingway\n",
            "Epoch:  65 | Train loss: 0.321 | Val loss: 0.404 | Gen:   onionionionionononon isway orky\n",
            "Epoch:  66 | Train loss: 0.317 | Val loss: 0.393 | Gen:   onininininininininii isway ory\n",
            "Epoch:  67 | Train loss: 0.321 | Val loss: 0.400 | Gen:  airwairwairwairwairi onionionionionionini isway orkingway\n",
            "Epoch:  68 | Train loss: 0.321 | Val loss: 0.400 | Gen:  airwairwairwairwaira onnnnnnnnnnnnnnnnnnn isway orkingway\n",
            "Epoch:  69 | Train loss: 0.317 | Val loss: 0.391 | Gen: ety airwairwairwairwairi ondiondiondiondiodio isway orkingway\n",
            "Epoch:  70 | Train loss: 0.314 | Val loss: 0.400 | Gen: ety airwairwairwairwaira ongcayEOSay isway ory\n",
            "Epoch:  71 | Train loss: 0.319 | Val loss: 0.398 | Gen: ethy  onionionnionionionio isway orkingway\n",
            "Epoch:  72 | Train loss: 0.314 | Val loss: 0.402 | Gen: ethay airwairwairwairwaira ongcayEOSay isway ory\n",
            "Epoch:  73 | Train loss: 0.311 | Val loss: 0.389 | Gen: ethay airwairwairwairwaira onionionionionionini isway ory\n",
            "Epoch:  74 | Train loss: 0.312 | Val loss: 0.402 | Gen: ethy airwairwairwairwaira onionionionoonononno isway orkingway\n",
            "Epoch:  75 | Train loss: 0.318 | Val loss: 0.403 | Gen: ety  onininininininininni  orkingy\n",
            "Epoch:  76 | Train loss: 0.325 | Val loss: 0.418 | Gen: ethay airwairwairwairwaira ondititititititititi isway ory\n",
            "Epoch:  77 | Train loss: 0.317 | Val loss: 0.394 | Gen: ety airwairwairwairwaira onionionionionionini isway ory\n",
            "Epoch:  78 | Train loss: 0.312 | Val loss: 0.390 | Gen: ety airwairwairwairwaira onionionioiioiioioio isway orky\n",
            "Epoch:  79 | Train loss: 0.310 | Val loss: 0.389 | Gen: ety airwairwairwairwaira onionionionionioniio isway orkingway\n",
            "Epoch:  80 | Train loss: 0.306 | Val loss: 0.400 | Gen:   ongcayEOSyitititititit isway ory\n",
            "Epoch:  81 | Train loss: 0.306 | Val loss: 0.390 | Gen: ety airwairwairwairwaira ondioidioidoiidododo isway orkingway\n",
            "Epoch:  82 | Train loss: 0.309 | Val loss: 0.395 | Gen: ethy  onionionionionionini isway orky\n",
            "Epoch:  83 | Train loss: 0.313 | Val loss: 0.392 | Gen: ety airwairwairwairwaira onionionioninninniin isway orky\n",
            "Epoch:  84 | Train loss: 0.308 | Val loss: 0.386 | Gen: ethy airwairwairwairwaira onininininininininin isway orky\n",
            "Epoch:  85 | Train loss: 0.306 | Val loss: 0.387 | Gen: ety airwairwairwairwaira onionionionionionini  \n",
            "Epoch:  86 | Train loss: 0.303 | Val loss: 0.385 | Gen: ethy airwairwairwairwaira ondityEOSyEOSyEOSyEOSyititit isway \n",
            "Epoch:  87 | Train loss: 0.304 | Val loss: 0.384 | Gen: ety airwairwairwairwaira onionionionionioniio  \n",
            "Epoch:  88 | Train loss: 0.303 | Val loss: 0.389 | Gen: ethy airwairwairwairwaira onititititititititit isway orky\n",
            "Epoch:  89 | Train loss: 0.300 | Val loss: 0.378 | Gen: ethy airwairwairwairwaira onionionionionioniin isway oay\n",
            "Epoch:  90 | Train loss: 0.302 | Val loss: 0.380 | Gen: ety airwairwairwairwaira  isway orkingwayEOSy\n",
            "Epoch:  91 | Train loss: 0.303 | Val loss: 0.384 | Gen: ety airwairwairwairwaira onionionionionioniin isway oray\n",
            "Epoch:  92 | Train loss: 0.303 | Val loss: 0.382 | Gen: ethy  onititititititititit isway oay\n",
            "Epoch:  93 | Train loss: 0.303 | Val loss: 0.390 | Gen:  airwairwairwairwaira onitninninninninnini isway ory\n",
            "Epoch:  94 | Train loss: 0.301 | Val loss: 0.384 | Gen: ety airwairwairwairwaira onionionionionioniin  orkingwayEOSy\n",
            "Epoch:  95 | Train loss: 0.303 | Val loss: 0.384 | Gen: ethy airwairwairwairwaira onionionionionionini isway orky\n",
            "Epoch:  96 | Train loss: 0.302 | Val loss: 0.395 | Gen: ety airwairwairwairwaira ondindindindindindin isway oay\n",
            "Epoch:  97 | Train loss: 0.298 | Val loss: 0.381 | Gen: ethy airwairwairwairwaira onionionioninninnini  orky\n",
            "Epoch:  98 | Train loss: 0.297 | Val loss: 0.382 | Gen: ethy airwairwairwairwaira onionionionionioniio  orkingwayEOSy\n",
            "Epoch:  99 | Train loss: 0.298 | Val loss: 0.381 | Gen: ethy airwairwairwairwaira onionionionionionini  ory\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethy airwairwairwairwaira onionionionionionini  ory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ULCMHm5ZF7vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "70b0b3b7-51dc-4d45-d932-b717c25e8274"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tety airway onditinicayy isway orkkngway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qbfZCByITOI6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention visualization"
      ]
    },
    {
      "metadata": {
        "id": "itCGMv3FdXsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TEST_WORD_ATTN = 'street'\n",
        "# TEST_WORD_ATTN = 'cake'\n",
        "# TEST_WORD_ATTN = 'drink'\n",
        "TEST_WORD_ATTN = 'aardvark'\n",
        "# TEST_WORD_ATTN = 'well-mannered'\n",
        "# TEST_WORD_ATTN = 'abcdefgh'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBv4QQuBiU-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "metadata": {
        "id": "aXvqoQYONMTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "3f53468b-fcc8-4e2e-cc23-83b013e24207"
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAGACAYAAAAjwCFIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'awrurrarkway'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "metadata": {
        "id": "xuOvxfA1NMz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "metadata": {
        "id": "HSSB4wd8-M7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1186
        },
        "outputId": "d56f962f-3cdf-41c0-f5ad-79b75caac43a"
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAGACAYAAAAnA4yEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZgAA61HMAADWo5gBAKxHMQMAWO//ARdGn0ufzpLMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAGACAYAAAAnA4yEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv0NaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAGACAYAAAAnA4yEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ardiwaay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    }
  ]
}